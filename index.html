<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="description" content="Oculus Render : 15-418 Final Project">

  <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/cpp.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <title>Conduit: Efficient Video Compression for Live VR Streaming</title>
</head>

<body>

<!-- HEADER -->
<div id="header_wrap" class="outer">
  <header class="inner">
    <a id="forkme_banner" href="https://github.com/avp/conduit">View on GitHub</a>

    <h1 id="project_title">Conduit</h1>

    <h2 id="project_tagline">
      Efficient Video Compression for Live VR Streaming
    </h2>

    <h3 id="project_author">
      Aakash Patel &amp; Gregory Rose
    </h3>

    <section id="downloads">
      <a class="zip_download_link" href="https://github.com/avp/conduit/zipball/master">
        Download this project as a .zip file
      </a>
      <a class="tar_download_link" href="https://github.com/avp/conduit/tarball/master">
        Download this project as a tar.gz file
      </a>
    </section>
  </header>
</div>

<!-- MAIN CONTENT -->
<div id="main_content_wrap" class="outer">
  <section id="main_content" class="inner">
    <h4>
      <a href="proposal.html">[Proposal]</a>
      &nbsp;
      <a href="checkpoint.html">[Checkpoint]</a>
    </h4>

    <h2>Problem</h2>

    <p>
      The experience of live events
      &mdash; concerts, sporting events, parades, and even being at Times Square on New Years &mdash;
      is powerful.
      Yet, these same events, viewed live at home on a TV, feel far more distant and less immersive.
      We believe that virtual reality (VR) can provide a far more immersive live experience than TV, by adding presence, the feeling that “you’re really there”.
      However, one of VR’s key advantages, the fact that you have the freedom to look anywhere in 360°, requires using a fully panoramic video.
      Panoramic videos are large, often 4K resolution (4096 pixels wide, 2160 pixels tall, depending on the standard).
      In addition, VR headsets typically work in 3D, meaning you need a panoramic video for each eye, so you’re streaming effectively a 4096x4096 video.
    </p>

    <p>
      This requires a very high amount of bandwidth &mdash; at least 25 Mbps,
      <a href="https://help.netflix.com/en/node/13444">according to Netflix</a>.
      However, according to Akamai, the average American internet speed is about 10 Mbps.
      In addition, while 4G internet can get almost to the 4K streaming point,
      <a href="https://bgr.com/2013/03/11/4g-network-speeds-368339/">at 20 Mbps down</a>,
      on average it's a lot lower.
      This effectively prohibits live streaming on mobile VR on all but some of the fastest internet connections in America.
    </p>

    <h2>Solution</h2>

    <p>
      Conduit is a project to reduce these bandwidth requirements
      for streaming live 4K panoramic video to a head-mounted display (HMD) like the <a
      href="https://www.oculus.com/">Oculus Rift</a>.
      We do this by using view-optimization, <strong>optimizing</strong> the video stream by compressing it,
      because we know what your <strong>view</strong> (where you’re looking) is,
      since the direction you’re looking at is reported by the HMD.
    </p>

    <p>
      Before streaming a frame over, we crop it to just the part you can see, instantly reducing total size by over 50%.
      Next, we use foveated rendering, a technique which takes advantage of the fact that the human eye does not have uniform resolution:
      our eyes have significantly higher resolution in the center region, called the fovea.
      Therefore, we downsample the outer regions, which comprise most of the image, saving even more space.
      However, because of this, you could move your head fast enough that it got outside the “view” region of the last streamed frame,
      and so we introduce multiple techniques to mitigate this by adaptively changing the compression parameters and taking advantage of the GPU to reduce latency.
    </p>

    <h2>Preliminary Results</h2>

    <p>
      We have a baseline working renderer that can read videos and render them to the Oculus.
      The frames are view optimized, and they account for the rotation of the viewer’s head.
      With our view optimization, we compress images to less than 20% of their original size.
    </p>

    <p>
      Additionally, we’ve made optimizations that, as much as we can measure now,
      reduce the average time per frame from 140ms as measured on a Samsung Chronos 7 with Nvidia GT 630M, to 90 ms.
    </p>

    <p>
      To simulate a client-server setup, we had our optimizer optimize a given frame,
      and then extract the unoptimized image to figure out how much time it would take.
      Optimization is a process that takes the viewer’s head position and the video frame,
      and gives back a smaller data structure that contains the optimized image.
      The optimized image would be sent over a network.
      The client would then extract the image and display it on the HMD.
    </p>

    <h2>Technologies</h2>

    <p>
      We used <b>OpenCV</b> to read frames from the MP4 video file. The library handles video reading and decoding using FFMpeg.
    </p>

    <p>
      For graphics, we used <b>OpenGL</b>, <b>GLEW</b>, and <b>GLUT</b>.
      OpenGL is used for texture mapping and rendering the cylinders to a screen.
      GLUT is able to render cylinders for us, which we can map textures onto.
      The textures are the frames from the video that we want to show to the user.
      The window manager is <b>SDL2</b>, which we use to look at the images generated by the graphics libraries.
    </p>

    <p>
      We also used the <b>Oculus Rift<</b> and its SDK. We sent our view-optimized images to the Oculus display where the video could be viewed.
    </p>

    <h2>Optimization</h2>

    <h3>Initial Version</h3>

    <p>
      Our first approach used a render loop like the following:
    </p>

<pre><code>OptimizedFrame optimize_frame(video_frame, viewer_data) {
  // crop_frame crops the frame to your FOV,
  // handling edge cases like if the viewer is looking
  // at the part of the panorama where the ends are stitched together.
  cropped_frame = crop_frame(video_frame, viewer_data.angle, 90);

  // extract the center 20 degrees
  left, center, right = horizontal_split(cropped_frame, 20);
  top, middle, bottom = vertical_split(center, 20);

  // shrink by a pre-defined constant factor of 5
  left = shrink(left);
  right = shrink(right);
  top = shrink(top);
  bottom = shrink(bottom);

  return OptimizedFrame(left, right, top, middle, bottom);
}

Frame decode_frame(optimized) {
  left = expand(optimized.left);
  right = expand(optimized.right);
  top = expand(optimized.top);
  bottom = expand(optimized.bottom);
  middle = optimized.middle;

  center = vertical_concat(top, middle, bottom);
  return horizontal_concat(left, center, right);
}

OpenGLTexture texture; // reference a texture on the GPU

while (true) {
  video_frame = get_video_frame();

  // We actually do each of these steps once for each eye,
  // having split video_frame into a "left eye" and "right eye"
  optimized_frame = optimize_frame(video_frame, viewer_data);
  decoded_frame = decode_frame(optimized_frame);

  copy_frame_to_texture(decoded_frame, texture);
  viewer_data = get_viewer_data_from_hmd();

  render_frame(decoded_frame, viewer_data);
}</code></pre>

    <!--<p>-->
      <!--First, let’s define “frame time” as the time to complete one iteration of this loop.-->
    <!--</p>-->

    <!--<p>-->
      <!--We profiled our code, and found that:-->
    <!--</p>-->

    <!-- TODO Profile results -->

    <h3>Optimization 1: Async Video Loading</h3>

    <p>
      Loading the video every single frame was taking a long time.
      Our first realization was that the work of loading the video frame is completely independent from the rest of the code!
      So while we’re optimizing, decoding, rendering, we can load more video frames in parallel.
      We created a second thread  to “buffer” the video, loading video frames continuously in the background,
      and putting them onto a queue, where the main thread could take them off.
    </p>

    <p>
      We limited the queue to 10 decoded frames, since decoded frames can be quite large (4096x4096 pixels * 24 bits/pixel for RGB 8-bit is 50.33 MB).
    </p>

    <p>
      <!--This cut down our frame time by XXXX-->
      We also found the video decoding ran much faster than the main loop, so the queue would quickly fill up and stay full.
    </p>

    <p>
      As a secondary optimization, we found that our dequeue function waited for a frame to be available.
      This meant that the main loop couldn’t run faster than video frames were decoded,
      and even if video frames were decoded slightly faster, if the video reader ever hung temporarily, it’d cause a hiccup in the main loop.
      In VR, it’s particularly important to have a fast main loop,
      since that’s where the frame is updated to reflect head-tracking, and high-latency and low framerates on head-tracking cause motion sickness.
      We then changed dequeue to just return <code>NULL</code> immediately if no frame was available,
      and in that case simply skipped updating the frame in the main loop.
      <!--This cut down our frame time by XXX.-->
    </p>

    <h3>Optimization 2: Simplifying the Optimizer</h3>

    <p>
    We implemented view optimization and measured results from that.
    </p>

    <p>
      <b>First Attempt:</b>
      First, we cropped the panoramic video frame that was read to 180&deg; in order to exclude regions that would be too far for the user to turn their head to see.
      Then, taking a yaw and a pitch, we compressed the image.
      To do this, we divided the image up horizontally: left, middle (the inner 20&deg;), and right.
      We scaled down the left and right sides by a factor of 5.
      The middle image was split into top, center and bottom sections.
      The top and bottom sections were similarly scaled down by a factor of 5.
      The middle section was left at full resolution.
    </p>

    <p>
      This way, we had a section in the middle of the image that the user could see as being completely in focus,
      and the rest of the image was blurry, in order to have a smaller size in memory &mdash;
      this optimized image would, in real world circumstances, be sent over a network.
      We measured the time that each of the steps in the view optimization took.
    </p>

    <p>
      <b>Problem:</b>
      On analysis, we saw that the slowest steps, by far, were the resizing steps of the optimizer.
      Each resizing section took over 100 times longer than any other step.
    </p>

    <p>
      <b>Solution:</b>
      Our initial attempt attempted to save space by splitting the image into exclusive regions, and operating on them individually.
      To restore, we would then scale each piece back up and concatenate them, first reassembling the vertical column, then reassembling the horizontal pieces.
      This was far too complicated, and error-prone, resulting in numerous bugs and edge cases.
      In addition, the multiple concatenations each copied every single pixel, meaning many pixels were copied multiple times, which is inefficient
    </p>

    <p>
      We fixed this by using a simpler approach, whereby we simply extract the center square at full resolution,
      and then take the entire image and scale it down.
      Then, to reconstruct it, we re-expand the cropped image to the original size,
      and paint the focused sub-image back in the center.
      While this does create redundant information for the center piece,
      the simpler approach overall reduced optimization time from ~26ms to ~18ms.
    </p>

    <h3>Optimization 3: Pixel Buffer Objects</h3>

    <p>
      We next address the issue of <code>copy_frame_to_texture</code>,
      which copies our decoded frame to the OpenGL texture, which is stored on the GPU.
      At first, we thought the primary issue was being bandwidth bound,
      but we profiled with the Nvidia X Server Info tool, and found that GPU bandwidth wasn’t even near being fully used.
    </p>

    <p>
      We instead found the problem to be waiting. We initially used <code>glTexImage2D</code>,
      which copies the texture to the GPU, every single time we got a new video frame.
      However, it turns out that <code>glTexImage2D</code> is blocking,
      and waits for the GPU to be available, and in particular done using the texture.
      In other words, it wastes a lot of time waiting around.
    <p>

    <p>
      We found a better solution to be using pixel-buffer objects (PBO).
      These allow you to copy your image data into a “pixel buffer”, and then tell the GPU to copy it into the texture, but it runs asynchronously.
      This is similar to asynchronous message passing from class.
      Using pixel-buffer objects reduced the time to copy textures from ~20ms to ~2ms,
      and since we do it once for each eye, it’s about a ~40ms win on total frame time
    <p>

    <p>
      <b>Further optimizations are pending writeup.</b>
    </p>

  </section>
</div>

<!-- FOOTER  -->
<div id="footer_wrap" class="outer">
  <footer class="inner">
    <p class="copyright">
      Conduit maintained by
      <a href="https://github.com/avp">avp</a>
      and
      <a href="https://github.com/grrosegr">grrosegr</a>
    </p>
  </footer>
</div>

</body>
</html>
