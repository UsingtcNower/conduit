<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="description" content="Oculus Render : 15-418 Final Project">

  <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

  <title>Conduit: Efficient Video Compression for Live VR Streaming</title>
</head>

<body>

<!-- HEADER -->
<div id="header_wrap" class="outer">
  <header class="inner">
    <a id="forkme_banner" href="https://github.com/avp/conduit">View on GitHub</a>

    <h1 id="project_title">Conduit</h1>

    <h2 id="project_tagline">
      Efficient Video Compression for Live VR Streaming
    </h2>

    <h3 id="project_author">
      Aakash Patel &amp; Gregory Rose
    </h3>

    <section id="downloads">
      <a class="zip_download_link" href="https://github.com/avp/conduit/zipball/master">
        Download this project as a .zip file
      </a>
      <a class="tar_download_link" href="https://github.com/avp/conduit/tarball/master">
        Download this project as a tar.gz file
      </a>
    </section>
  </header>
</div>

<!-- MAIN CONTENT -->
<div id="main_content_wrap" class="outer">
  <section id="main_content" class="inner">
    <h4>
      <a href="proposal.html">[Proposal]</a>
      &nbsp;
      <a href="checkpoint.html">[Checkpoint]</a>
    </h4>

    <h2>Problem</h2>

    <p>
      The experience of live events
      &mdash; concerts, sporting events, parades, and even being at Times Square on New Years &mdash;
      is powerful.
      Yet, these same events, viewed live at home on a TV, feel far more distant and less immersive.
      We believe that virtual reality (VR) can provide a far more immersive live experience than TV, by adding presence, the feeling that “you’re really there”.
      However, one of VR’s key advantages, the fact that you have the freedom to look anywhere in 360°, requires using a fully panoramic video.
      Panoramic videos are large, often 4K resolution (4096 pixels wide, 2160 pixels tall, depending on the standard).
      In addition, VR headsets typically work in 3D, meaning you need a panoramic video for each eye, so you’re streaming effectively a 4096x4096 video.
    </p>

    <p>
      This requires a very high amount of bandwidth &mdash; at least 25 Mbps,
      <a href="https://help.netflix.com/en/node/13444">according to Netflix</a>.
      However, according to Akamai, the average American internet speed is about 10 Mbps.
      In addition, while 4G internet can get almost to the 4K streaming point,
      <a href="https://bgr.com/2013/03/11/4g-network-speeds-368339/">at 20 Mbps down</a>,
      on average it's a lot lower.
      This effectively prohibits live streaming on mobile VR on all but some of the fastest internet connections in America.
    </p>

    <h2>Solution</h2>

    <p>
      Conduit is a project to reduce these bandwidth requirements
      for streaming live 4K panoramic video to a head-mounted display (HMD) like the <a
      href="https://www.oculus.com/">Oculus Rift</a>.
      We do this by using view-optimization, <strong>optimizing</strong> the video stream by compressing it,
      because we know what your <strong>view</strong> (where you’re looking) is,
      since the direction you’re looking at is reported by the HMD.
    </p>

    <p>
      Before streaming a frame over, we crop it to just the part you can see, instantly reducing total size by over 50%.
      Next, we use foveated rendering, a technique which takes advantage of the fact that the human eye does not have uniform resolution:
      our eyes have significantly higher resolution in the center region, called the fovea.
      Therefore, we downsample the outer regions, which comprise most of the image, saving even more space.
      However, because of this, you could move your head fast enough that it got outside the “view” region of the last streamed frame,
      and so we introduce multiple techniques to mitigate this by adaptively changing the compression parameters and taking advantage of the GPU to reduce latency.
    </p>

    <h2>Preliminary Results</h2>

    <p>
      <img src="images/fountain-1.png" width="100%"/>
    </p>

    <p>
      We have a baseline working renderer that can read videos and render them to the Oculus.
      The frames are view optimized, and they account for the rotation of the viewer’s head.
      With our view optimization, we compress images to less than 20% of their original size.
      Adequate performance was achieved by offloading video reading and decoding to a separate thread.
      We then used a work queue to buffer the frames, and the Oculus only loads new frames from the buffer, preventing stalls.
    </p>

    <h3>Approach</h3>

    <p>
      To simulate a client-server setup, we had our optimizer optimize a given frame, and then extract the unoptimized image to figure out how much time it would take.
      Optimization is a process that takes the viewer’s head position and the video frame, and gives back a smaller data structure that contains the optimized image.
      The optimized image would be sent over a network.
      The client would then extract the image and display it on the HMD.
    </p>

    <p>
      Our first approach was something like:
    </p>

    <pre><code>while (true) {
  video_frame = get_video_frame();
  viewer_data = get_viewer_data_from_hmd();
  optimized_frame = optimize_frame(video_frame, viewer_data);
  decoded_frame = decode_frame(optimized_frame);

  render_frame(optimized_frame, viewer_data);
}</code></pre>

    <p>
      However, this tied the framerate to the amount of time to optimize a frame.
      This was bad, because it resulted in a substantially lower frame rate for the head-tracking, which is what makes people sick.
      You move your head, and then see your vision update after 100ms.
      Instead, we decided that even if the video frame takes a while,
      there’s no reason we can’t keep updating the scene with your viewer_data
      (notice viewer_data, containing your head position and orientation, is an argument to render_frame--it tells the code where the camera is).
    </p>

    <p>
      Therefore, we decoupled it using a producer-consumer model with a queue.
      The main render thread is now only responsible for rendering the frame.
      Another thread keeps reading and optimizing the video, and it puts each finished frame into a queue.
      The main thread updates its video frame out of the queue if one is available, but if not, it keeps showing the last video frame.
      In addition, rendering the scene and decoding the frame are independent parts, so we improve performance by running them in parallel.
      Together, these all make a better user experience.
    </p>

    <p>
      Note that the second thread keeps a copy of viewer data in order to perform the view-optimization, and the main thread updates it periodically. Now, it’s more like:
    </p>

    <pre><code>// Thread 1
Queue frameQueue;
Frame frame = frameQueue.dequeue();
ViewerData viewerData;

while (true) {
  if (frameQueue.hasNewFrame())
    frame = frameQueue.dequeue();
  viewerData = get_viewer_data_from_hmd();

  render_frame(optimized_frame, viewer_data);
}

// Thread 2
ViewerData viewerData;
while (videoFrameAvailable()) {
  frame = readFrame();
  frame = deoptimizeFrame(optimizeFrame(frame, viewerData));
  frameQueue.enqueue(frame);
}</code></pre>

    <p>
      Adequate performance was achieved by offloading video reading and decoding to a separate thread.
      We then used a work queue to buffer the frames, and the oculus only loads new frames from the buffer, preventing stalls.
    </p>

  </section>
</div>

<!-- FOOTER  -->
<div id="footer_wrap" class="outer">
  <footer class="inner">
    <p class="copyright">
      Conduit maintained by
      <a href="https://github.com/avp">avp</a>
      and
      <a href="https://github.com/grrosegr">grrosegr</a>
    </p>
  </footer>
</div>

</body>
</html>
