<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="description" content="Oculus Render : 15-418 Final Project">

  <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

  <title>Conduit</title>
</head>

<body>

<!-- HEADER -->
<div id="header_wrap" class="outer">
  <header class="inner">
    <a id="forkme_banner" href="https://github.com/avp/conduit">View on GitHub</a>

    <h1 id="project_title">Conduit</h1>

    <h2 id="project_tagline">Aakash Patel &amp; Gregory Rose</h2>

    <section id="downloads">
      <a class="zip_download_link" href="https://github.com/avp/conduit/zipball/master">
        Download this project as a .zip file
      </a>
      <a class="tar_download_link" href="https://github.com/avp/conduit/tarball/master">
        Download this project as a tar.gz file
      </a>
    </section>
  </header>
</div>

<!-- MAIN CONTENT -->
<div id="main_content_wrap" class="outer">
  <section id="main_content" class="inner">
    <h2>Proposal</h2>

    <h3>Summary</h3>

    <p>
      Conduit is a project to reduce bandwidth requirements for streaming live 4K panoramic video to a HMD (head-mounted display, e.g. Oculus Rift). To do this, we will take advantage of the fact that the human eye has a much higher resolution in the center than in the outside, and transcode video on the outer part of your vision at a lower quality. Our goal is to do this with minimal latency, and maximum stream-per-system performance.
    </p>

    <h3>Background</h3>
    <!-- TODO: I've added a few details, but there's definitely more to say -Greg -->
    <!-- TODO: These are the words to use, but we might want to add some formatting -->

    <h4>Definitions</h4>

    <p>
      <strong>Source video:</strong>
      The original raw 4K panoramic video stream we want to make available through HMDs.
    </p>

    <p>
      <strong>View-optimized video:</strong>
      The source video, cropped to show just the parts the user is looking at (plus some padding if they move).
      This is optimized to display lower quality video to lower resolution parts of the human eye.
    </p>

    <p>
      <strong>HMD video:</strong>
      The video cropped to just the part the HMD is looking at, re-projected for both eyes, and formatted via barrel distortion for the Oculus.
    </p>

    <p>
      <strong>Client:</strong>
      A personal computer or phone receiving the video stream,
      and performing some processing on the stream.
    </p>

    <h4>Project</h4>

    <p>
      This project revolves around the idea of a bandwidth-latency tradeoff, a challenge in parallelizing programs. We spend extra time processing the video to make it smaller, in order to reduce bandwidth requirements.
    </p>

    <p>
      The naive approach to getting streaming VR video would be to stream the entire 4K source video, and process it on the client.
      However, <a href="https://help.netflix.com/en/node/13444">Netflix recommends</a>
      an Internet connection speed of 25 Mbps for streaming a 4K video.
      According to Akamai, the average American internet speed is about 10 Mbps.
      In addition, while 4G internet can get almost to the 4K streaming point,
      <a
        href="https://bgr.com/2013/03/11/4g-network-speeds-368339/">at 20 Mbps down</a>, on average it's a lot lower.
      Of course, mobile networks aren't built to handle
      <i>everyone</i> using that much data all the time.
    </p>

    <p>
      If this technique is successful (and fully realizing it goes well beyond the scope of this project),
      it may make it practical to attend events live performances and events like concerts and sports games in VR,
      even from a mobile device, by reducing the bandwidth requirement.
    </p>

    <h3>Challenge</h3>

    <p>
      There are many challenges to this project. First, neither of us have worked with developing low-level code for VR, or video streaming and video codecs.
    </p>

    <h4>Workload</h4>
    <!-- Still not sure at all on this -->
    <p>
      In terms of dependencies, within a single frame, things are mostly independent,
      but across frames, if we make a change to one frame, the next one needs to be somewhat consistent with it.
      This will matter if we try to go out of order.
    </p>

    <p>
      We'll use both a lot of bandwidth, and a lot of compute, as 4K video contains a lot of data, and video compression does a lot of work with that data.
      On the client side, it is fundamentally a rendering problem, which can be done well in parallel.
    </p>

    <h4>Constraints</h4>

    <p>
      During the video processing phase, the areas of the video we'll be compressing will be varying, and often strangely segmented.
      In other words, they won't be nice rectangles, but irregular shapes that overlap, causing a lot of false sharing with the cache.
    </p>

    <h3>Resources</h3>

    <p>
      We'll start from scratch, but ultimately probably need to use video compression libraries.
      We'll use the Oculus SDK. Oculus also has C++ example code for a scene in case we get stuck.
    </p>

    <h3>Goals &amp; Deliverables</h3>

    <ol>
      <li>Render the panoramic video to the Oculus</li>
      <li>View optimize the stream</li>
    </ol>

    <h3>Platform Choice</h3>

    <h3>Schedule</h3>
  </section>
</div>

<!-- FOOTER  -->
<div id="footer_wrap" class="outer">
  <footer class="inner">
    <p class="copyright">
      Conduit maintained by
      <a href="https://github.com/avp">avp</a>
      and
      <a href="https://github.com/grrosegr">grrosegr</a>
    </p>
  </footer>
</div>


</body>
</html>
